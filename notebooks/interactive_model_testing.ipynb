{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "072a14f1-7b86-45e7-b2e1-969abe03de47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflite_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e39147fe-21d9-42b3-99ff-eb7547643221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 84, 8400)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tflite_runtime.interpreter as tflite\n",
    "\n",
    "# Delegates/Executes all operations supported by Arm NN to/with Arm NN\n",
    "interpreter = tflite.Interpreter(model_path=\"/home/pi/exp/exp1/models/yolov5nu_float32.tflite\", \n",
    "                                 num_threads=4)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "# Print out result\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5d109b85-0d76-4dd2-9a4a-c8f2369f645d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ultralytics YOLO ðŸš€, AGPL-3.0 license\n",
    "\n",
    "import argparse\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tflite_runtime import interpreter as tflite\n",
    "from letterletterbox import LLetterBox\n",
    "\n",
    "\n",
    "class Yolov8TFLite:\n",
    "    def __init__(self, tflite_model, size, confidence_thres, iou_thres):\n",
    "        self.size = size\n",
    "        self.tflite_model = tflite_model\n",
    "        self.confidence_thres = confidence_thres\n",
    "        self.iou_thres = iou_thres\n",
    "        interpreter = tflite.Interpreter(model_path=self.tflite_model, \n",
    "                                        num_threads=4)\n",
    "        self.model = interpreter\n",
    "        self.model.allocate_tensors()\n",
    "\n",
    "        # Load the class names from the COCO dataset\n",
    "\n",
    "        # Generate a color palette for the classes\n",
    "        self.color_palette = np.random.uniform(0, 255, size=(100, 3))\n",
    "\n",
    "    def draw_detections(self, img, box, score, class_id):\n",
    "        # Extract the coordinates of the bounding box\n",
    "        x1, y1, w, h = box\n",
    "\n",
    "        # Retrieve the color for the class ID\n",
    "        color = self.color_palette[class_id]\n",
    "\n",
    "        # Draw the bounding box on the image\n",
    "        cv2.rectangle(img, (int(x1), int(y1)), (int(x1 + w), int(y1 + h)), color, 2)\n",
    "\n",
    "        # Create the label text with class name and score\n",
    "        label = f\"det: {score:.2f}\"\n",
    "\n",
    "        # Calculate the dimensions of the label text\n",
    "        (label_width, label_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "\n",
    "        # Calculate the position of the label text\n",
    "        label_x = x1\n",
    "        label_y = y1 - 10 if y1 - 10 > label_height else y1 + 10\n",
    "\n",
    "        # Draw a filled rectangle as the background for the label text\n",
    "        cv2.rectangle(\n",
    "            img,\n",
    "            (int(label_x), int(label_y - label_height)),\n",
    "            (int(label_x + label_width), int(label_y + label_height)),\n",
    "            color,\n",
    "            cv2.FILLED,\n",
    "        )\n",
    "\n",
    "        # Draw the label text on the image\n",
    "        cv2.putText(img, label, (int(label_x), int(label_y)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    def preprocess(self):\n",
    "        self.img = cv2.imread(self.input_image) if  not isinstance(self.input_image, np.ndarray) else self.input_image\n",
    "        self.img_height, self.img_width = self.img.shape[:2]\n",
    "        letterbox = LetterBox(new_shape=self.size, auto=False, stride=32)\n",
    "        image = np.stack([letterbox(image=self.img)])\n",
    "        image = image[..., ::-1].transpose((0, 3, 1, 2))\n",
    "        image = np.ascontiguousarray(image).astype(np.float32)\n",
    "        return image / 255\n",
    "\n",
    "    def postprocess(self, input_image, output):\n",
    "        \"\"\"\n",
    "        Performs post-processing on the model's output to extract bounding boxes, scores, and class IDs.\n",
    "\n",
    "        Args:\n",
    "            input_image (numpy.ndarray): The input image.\n",
    "            output (numpy.ndarray): The output of the model.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: The input image with detections drawn on it.\n",
    "        \"\"\"\n",
    "        pred = np.transpose(output[0]) \n",
    "        x = pred[:, 0] - pred[:, 2] / 2\n",
    "        y = pred[:, 1] - pred[:, 3] / 2\n",
    "        w = pred[:, 2]\n",
    "        h = pred[:, 3]\n",
    "        boxes = np.vstack([x, y, w, h]).T\n",
    "        class_ids = np.argmax(pred[:, 4:], axis=1)\n",
    "        scores = np.max(pred[:, 4:], axis=1)\n",
    "\n",
    "        indices = cv2.dnn.NMSBoxes(boxes, scores, self.confidence_thres, self.iou_thres)\n",
    "\n",
    "        import time\n",
    "        start = time.time()\n",
    "        for i in indices:\n",
    "            # Get the box, score, and class ID corresponding to the index\n",
    "            box = boxes[i]\n",
    "            gain = min(img_width / self.img_width, img_height / self.img_height)\n",
    "            pad = (\n",
    "                round((img_width - self.img_width * gain) / 2 - 0.1),\n",
    "                round((img_height - self.img_height * gain) / 2 - 0.1),\n",
    "            )\n",
    "            box[0] = (box[0] - pad[0]) / gain\n",
    "            box[1] = (box[1] - pad[1]) / gain\n",
    "            box[2] = box[2] / gain\n",
    "            box[3] = box[3] / gain\n",
    "            score = scores[i]\n",
    "            class_id = class_ids[i]\n",
    "\n",
    "            if score > 0.25:\n",
    "                # Draw the detection on the input image\n",
    "                self.draw_detections(input_image, box, score, class_id)\n",
    "        print(time.time() - start)\n",
    "        return input_image\n",
    "\n",
    "    def main(self, input_image):\n",
    "        \"\"\"\n",
    "        Performs inference using a TFLite model and returns the output image with drawn detections.\n",
    "\n",
    "        Returns:\n",
    "            output_img: The output image with drawn detections.\n",
    "        \"\"\"\n",
    "        self.input_image = input_image \n",
    "        # Create an interpreter for the TFLite model\n",
    "\n",
    "        interpreter = self.model\n",
    "        # Get the model inputs\n",
    "        input_details = interpreter.get_input_details()\n",
    "        output_details = interpreter.get_output_details()\n",
    "\n",
    "        # Store the shape of the input for later use\n",
    "        input_shape = input_details[0][\"shape\"]\n",
    "        self.input_width = input_shape[1]\n",
    "        self.input_height = input_shape[2]\n",
    "\n",
    "        # Preprocess the image data\n",
    "        img_data = self.preprocess()\n",
    "        self.img_data = img_data\n",
    "        # img_data = img_data.cpu().numpy()\n",
    "        # Set the input tensor to the interpreter\n",
    "        print(img_data.shape)\n",
    "        img_data = img_data.transpose((0, 2, 3, 1))\n",
    "\n",
    "        scale, zero_point = input_details[0][\"quantization\"]\n",
    "        interpreter.set_tensor(input_details[0][\"index\"], img_data)\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "        output[:, [0, 2]] *= img_width\n",
    "        output[:, [1, 3]] *= img_height\n",
    "        return self.postprocess(self.img, output)\n",
    "\n",
    "\n",
    "# Create an argument parser to handle command-line arguments\n",
    "\n",
    "# Create an instance of the Yolov8TFLite class with the specified arguments\n",
    "detection = Yolov8TFLite(\"/home/pi/exp/exp1/models/yolov5nu_float32.tflite\", (640, 640), 0.5, 0.5)\n",
    "\n",
    "# Perform object detection and obtain the output image\n",
    "\n",
    "img = cv2.imread(\"../data/images/coco_bike.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2c3289a4-7430-4560-ab9e-1bfb115c7aa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 640, 640)\n",
      "(1, 3, 640, 640)\n",
      "(1, 3, 640, 640)\n",
      "(1, 3, 640, 640)\n",
      "(1, 3, 640, 640)\n",
      "(1, 3, 640, 640)\n",
      "(1, 3, 640, 640)\n",
      "(1, 3, 640, 640)\n",
      "178 ms Â± 9.71 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "output_image = detection.main(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6343ccda-0f84-42d3-a7e2-c0965161886c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mort\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInferenceSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath_or_bytes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | bytes | os.PathLike'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msess_options\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Sequence[onnxruntime.SessionOptions] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mproviders\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Sequence[str | tuple[str, dict[Any, Any]]] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprovider_options\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Sequence[dict[Any, Any]] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'None'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      This is the main class used to run a model.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       ":param path_or_bytes: Filename or serialized ONNX or ORT format model in a byte string.\n",
       ":param sess_options: Session options.\n",
       ":param providers: Optional sequence of providers in order of decreasing\n",
       "    precedence. Values can either be provider names or tuples of\n",
       "    (provider name, options dict). If not provided, then all available\n",
       "    providers are used with the default precedence.\n",
       ":param provider_options: Optional sequence of options dicts corresponding\n",
       "    to the providers listed in 'providers'.\n",
       "\n",
       "The model type will be inferred unless explicitly set in the SessionOptions.\n",
       "To explicitly set:\n",
       "\n",
       "::\n",
       "\n",
       "    so = onnxruntime.SessionOptions()\n",
       "    # so.add_session_config_entry('session.load_model_format', 'ONNX') or\n",
       "    so.add_session_config_entry('session.load_model_format', 'ORT')\n",
       "\n",
       "A file extension of '.ort' will be inferred as an ORT format model.\n",
       "All other filenames are assumed to be ONNX format models.\n",
       "\n",
       "'providers' can contain either names or names and options. When any options\n",
       "are given in 'providers', 'provider_options' should not be used.\n",
       "\n",
       "The list of providers is ordered by precedence. For example\n",
       "`['CUDAExecutionProvider', 'CPUExecutionProvider']`\n",
       "means execute a node using `CUDAExecutionProvider`\n",
       "if capable, otherwise execute using `CPUExecutionProvider`.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/exp/exp1/venv/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "ort.InferenceSession?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a59e93c-964d-4f3b-a5c1-59df412ae8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pi/exp/exp1/src\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0128a0b6-e81e-4450-962a-2033057ad461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov5nu.pt to 'yolov5nu.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.27M/5.27M [00:00<00:00, 16.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.42 ðŸš€ Python-3.11.2 torch-2.2.2 CPU (Cortex-A76)\n",
      "YOLOv5n summary (fused): 193 layers, 2649200 parameters, 0 gradients, 7.7 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov5nu.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.0 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 3.5s, saved as 'yolov5nu.onnx' (10.3 MB)\n",
      "\n",
      "Export complete (9.0s)\n",
      "Results saved to \u001b[1m/home/pi/exp/exp1/src\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolov5nu.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolov5nu.onnx imgsz=640 data=coco.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yolov5nu.onnx'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov5nu.pt')\n",
    "\n",
    "# Export the model to NCNN format\n",
    "model.export(format='onnx', optimize=True, simplify=False) # creates '/yolov8n_ncnn_model'\n",
    "\n",
    "\n",
    "\n",
    "# Run inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "119ae101-cf51-4806-9eea-1dd604e586b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/pi/exp/exp1/src/bus.jpg: 640x640 5 persons, 1 bus, 279.5ms\n",
      "Speed: 10.3ms preprocess, 279.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/pi/exp/exp1/src/bus.jpg: 640x640 5 persons, 1 bus, 252.2ms\n",
      "Speed: 8.4ms preprocess, 252.2ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/pi/exp/exp1/src/bus.jpg: 640x640 5 persons, 1 bus, 247.7ms\n",
      "Speed: 8.4ms preprocess, 247.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/pi/exp/exp1/src/bus.jpg: 640x640 5 persons, 1 bus, 175.6ms\n",
      "Speed: 8.1ms preprocess, 175.6ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/pi/exp/exp1/src/bus.jpg: 640x640 5 persons, 1 bus, 178.4ms\n",
      "Speed: 8.3ms preprocess, 178.4ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/pi/exp/exp1/src/bus.jpg: 640x640 5 persons, 1 bus, 178.6ms\n",
      "Speed: 8.1ms preprocess, 178.6ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/pi/exp/exp1/src/bus.jpg: 640x640 5 persons, 1 bus, 177.7ms\n",
      "Speed: 8.1ms preprocess, 177.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Found https://ultralytics.com/images/bus.jpg locally at bus.jpg\n",
      "image 1/1 /home/pi/exp/exp1/src/bus.jpg: 640x640 5 persons, 1 bus, 177.6ms\n",
      "Speed: 8.3ms preprocess, 177.6ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "231 ms Â± 32.9 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "results = ncnn_model('https://ultralytics.com/images/bus.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb81ea-9e67-4d97-93ff-280fc7040d66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
